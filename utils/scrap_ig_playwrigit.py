import aiohttp
import re, os, sys, time, itertools
from pathlib import Path
from urllib.parse import urljoin, urlparse, urlsplit, urlunsplit
from typing import List, Dict, Any, Optional, Set, Tuple
from playwright.async_api import async_playwright
import subprocess
import shutil
import asyncio
import requests
import datetime

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config.config import settings

IMAGE_DIR2 = settings['IMAGE_DIR2']
# print(IMAGE_DIR2)
BASE_SAVE_DIR = IMAGE_DIR2
# BASE_SAVE_DIR = r"D:\temp"

# ======== ì„¤ì • ========
USER_DATA_DIR = str(Path("./ig_profile-0").resolve())  # ì„¸ì…˜ ì €ì¥ (2íšŒì°¨ë¶€í„° ìë™ ë¡œê·¸ì¸)  # fx014
USER_DATA_DIR = str(Path("./ig_profile-1").resolve())  # fx015
# USER_DATA_DIR = str(Path("./ig_profile-2").resolve())  # fx016
HEADLESS = False

USERNAME = settings['SCRAP_USERNAME']   # ì¸ìŠ¤íƒ€ ë¡œê·¸ì¸ ê³„ì •
PASSWORD = settings['SCRAP_PASSWORD']   # ë¹„ë°€ë²ˆí˜¸

# USERNAME = ""   # ì¸ìŠ¤íƒ€ ë¡œê·¸ì¸ ê³„ì •
# PASSWORD = ""   # ë¹„ë°€ë²ˆí˜¸

# ìŠ¤í¬ë¡¤/ì†ë„
SCROLL_PAUSE = 1.8
MAX_SCROLLS = 30001
DELAY_SECOND = 2.0
DELAY_MINUTE = 60 * 5
ALREADY_COLLECTED_COUNT = 50

ACCOUNTS = [

]
# ACCOUNTS = ["fkaus014"]  # ìŠ¤í¬ë© ëŒ€ìƒ ê³„ì • ë°°ì—´

TEST_LINKS = [

]






# â”€â”€ CDN/ì‘ë‹µ í•„í„° ì„¤ì •: ë¦¬ì „/ì„¸ê·¸ë¨¼íŠ¸ ë²„ì „ ë‹¤ì–‘ì„± ëŒ€ì‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CDN_HOST_RE   = re.compile(r"^https://scontent-[a-z0-9\-]+\.cdninstagram\.com/") # scontent-ssn1-1 ë“±
CDN_PATH_ALLOW= re.compile(r"/o1/")   # # /o1/ ê²½ë¡œ í¬í•¨ (t2, t16 ë“± ì„¸ë¶€ ë²„ì „ì€ ë‹¤ì–‘), í•„ìš” ì‹œ |/o0/ ë“± ì¶”ê°€
MIN_GOOD_VIDEO_BYTES = 500_000        # 0.5MB ì´ìƒë§Œ í›„ë³´


# ë™ì‹œ ë‹¤ìš´ë¡œë“œ ì œí•œ
MAX_CONCURRENCY = 4

# ======== ìœ í‹¸ ========
def sanitize_filename(name: str) -> str:
    return re.sub(r'[\\/:*?"<>|]+', "_", name)

def ensure_dirs(base: str, account: str) -> Dict[str, str]:
    base_acc = os.path.join(base, account)
    img_dir = os.path.join(base_acc, "images")
    reel_dir = os.path.join(base_acc, "reels")
    os.makedirs(img_dir, exist_ok=True)
    os.makedirs(reel_dir, exist_ok=True)
    return {"base": base_acc, "images": img_dir, "reels": reel_dir}

def guess_ext_from_url_or_type(url: str, content_type: Optional[str]) -> str:
    if content_type:
        if "image/" in content_type:
            return ".jpg" if "jpeg" in content_type else f".{content_type.split('/')[1].split(';')[0]}"
        if "video/" in content_type:
            return ".mp4"
    # fallback to URL path
    path = urlparse(url).path
    ext = os.path.splitext(path)[1]
    if ext:
        return ext.split("?")[0]
    # ultimate fallback
    if "/video" in url:
        return ".mp4"
    return ".jpg"

def is_post_or_reel(url: str) -> bool:
    try:
        path = url.split("?")[0]
        return "/p/" in path or "/reel/" in path
    except Exception:
        return False

def is_media_response(resp) -> bool:
    try:
        ct = (resp.headers or {}).get("content-type", "")
        return ("image" in ct or "video" in ct) and not resp.url.startswith("blob:")
    except Exception:
        return False

# --- helper: ffprobeë¡œ ë¹„ë””ì˜¤ ê¸¸ì´(ì´ˆ) ê°€ì ¸ì˜¤ê¸° ---
def get_video_duration_sec(path: str):
    """
    ffprobe(FFmpeg) í•„ìš”. ì„±ê³µ ì‹œ ê¸¸ì´(ì´ˆ) float, ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    if shutil.which("ffprobe") is None:
        # ffprobe ë¯¸ì„¤ì¹˜
        return None
    try:
        # durationë§Œ ê¹”ë”í•˜ê²Œ ì¶œë ¥
        # ì°¸ê³ : ì¼ë¶€ íŒŒì¼ì€ format.duration ëŒ€ì‹  stream.durationì´ í•„ìš”í•  ìˆ˜ ìˆì–´ ë‹¨ìˆœ ì¶œë ¥ ì‚¬ìš©
        res = subprocess.run(
            [
                "ffprobe", "-v", "error",
                "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1",
                path
            ],
            capture_output=True, text=True, timeout=10
        )
        out = (res.stdout or "").strip()
        if not out:
            return None
        return float(out)
    except Exception:
        return None

def canonical_cdn_key(u: str) -> str:
    """
    ì¿¼ë¦¬ìŠ¤íŠ¸ë§ì€ ë¬´ì‹œí•˜ê³ , host+pathë§Œ í‚¤ë¡œ ì‚¬ìš© (ë™ì¼ ë¦¬ì†ŒìŠ¤ ì¤‘ë³µ ì œê±°).
    ì˜ˆ: https://scontent-.../o1/v/t2/abc.mp4?efg=... -> scontent-.../o1/v/t2/abc.mp4
    """
    p = urlsplit(u)
    return f"{p.netloc}{p.path}"

def extract_account_and_type(url):
    # URL íŒŒì‹±
    parsed = urlparse(url)
    # print(parsed)
    # ParseResult(scheme='https', netloc='www.instagram.com', path='/fkaus014/p/DO27U_DDw63/')

    # 1) ê¸°ë³¸ ë„ë©”ì¸ (https://www.instagram.com)
    base_url = f"{parsed.scheme}://{parsed.netloc}"

    # 2) path ë¶€ë¶„ ë‚˜ëˆ„ê¸°
    parts = [p for p in parsed.path.split("/") if p]  # ë¹ˆ ë¬¸ìì—´ ì œê±°
    # parts = ['fkaus014', 'p', 'DO27U_DDw63']

    # username = parts[0]  # fkaus014
    # post_type = parts[1] # p
    # post_id   = parts[2] # DO27U_DDw63

    post_type = parts[0] # p
    post_id   = parts[1] # DO27U_DDw63

    return {
        "type": post_type
    }

# ======== ë¸Œë¼ìš°ì € ì¡°ì‘ ========
async def ensure_login(page):
    await page.goto("https://www.instagram.com/", wait_until="domcontentloaded")
    await asyncio.sleep(4)
    # ë¡œê·¸ì¸ í¼ ë³´ì´ë©´ ë¡œê·¸ì¸
    login_user = page.locator("input[name='username']")
    login_pass = page.locator("input[name='password']")
    if await login_user.count() and await login_pass.count():
        await login_user.fill(USERNAME)
        await login_pass.fill(PASSWORD)
        await login_pass.press("Enter")
        await page.wait_for_load_state("networkidle")
        await asyncio.sleep(10)
        # íŒì—… ë‹«ê¸°
        for txt in ["ë‚˜ì¤‘ì— í•˜ê¸°", "Not Now"]:
            btn = page.locator(f"button:has-text('{txt}')")
            if await btn.count():
                await btn.click()
                await asyncio.sleep(1)

async def go_to_profile(page, handle: str):
    url = f"https://www.instagram.com/{handle.strip('/')}/"
    await page.goto(url, wait_until="domcontentloaded")
    await page.wait_for_selector("main", timeout=15000)


POST_PREFIXES = {"p", "reel", "tv", "stories"}  # IGTV í¬í•¨(í•„ìš” ì—†ìœ¼ë©´ ì œê±°)

def normalize_ig_post_url(url: str) -> str:
    """
    https://www.instagram.com/<user>/p/<code>  -> https://www.instagram.com/p/<code>
    https://www.instagram.com/<user>/reel/<code> -> https://www.instagram.com/reel/<code>
    ì´ë¯¸ /p/, /reel/, /tv/ ë¡œ ì‹œì‘í•˜ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€.
    ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ë³´ì¡´, ìƒëŒ€ê²½ë¡œë„ í—ˆìš©.
    """
    base = "https://www.instagram.com"
    s = urlsplit(urljoin(base, url))  # ìƒëŒ€ê²½ë¡œ ëŒ€ë¹„
    parts = [seg for seg in s.path.split("/") if seg]  # ["user","p","CODE"]

    if len(parts) >= 2 and parts[0] not in POST_PREFIXES and parts[1] in POST_PREFIXES:
        # ì²« ì„¸ê·¸ë¨¼íŠ¸ê°€ ìœ ì €ëª…ì´ê³ , ê·¸ ë‹¤ìŒì´ p/reel/tv ì¸ ì „í˜•ì  íŒ¨í„´ â†’ ìœ ì €ëª… ì œê±°
        parts = parts[1:]

    new_path = "/" + "/".join(parts) if parts else "/"
    return urlunsplit((s.scheme, s.netloc, new_path, s.query, s.fragment))


async def collect_post_links(page, max_scrolls=MAX_SCROLLS, pause=SCROLL_PAUSE) -> List[str]:
    """í”„ë¡œí•„ í˜ì´ì§€ì—ì„œ ìŠ¤í¬ë¡¤í•˜ë©° /p/, /reel/ ë§í¬ ìˆ˜ì§‘ (ìƒëŒ€ê²½ë¡œ â†’ ì ˆëŒ€ê²½ë¡œ)"""
    links = []
    post_links: Set[str] = set()
    # stable_rounds = 0
    # last_count = 0
    already_collected_count = 0
    await page.wait_for_selector("main", timeout=20000)
    await asyncio.sleep(4)

    last_height = await page.evaluate("document.body.scrollHeight")

    for _ in range(max_scrolls):
        anchors = await page.locator('a[href*="/p/"], a[href*="/reel/"]').element_handles()
        if len(anchors) == 0:
            print('  â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Account is not valid â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… ')
        for a in anchors:
            href = await a.get_attribute("href")
            if not href:
                continue

            # ì ˆëŒ€ê²½ë¡œí™”
            if href.startswith("/"):
                # href: /ê³„ì •/p/postId/
                parseResult = urlparse(normalize_ig_post_url(href))
                origin_href = parseResult.path

                href = urljoin("https://www.instagram.com", href)

            # acount ì„¸ê·¸ë¨¼íŠ¸ ì œê±°
            # href = normalize_ig_post_url(href)

            if is_post_or_reel(href):
                if already_collected_count > ALREADY_COLLECTED_COUNT:
                    rev_links = links[::-1]   # slicing, ì›ë³¸ ë³´ì¡´
                    return rev_links

                url = "https://chickchick.shop/func/scrap-posts?urls="+origin_href
                res = requests.get(url)
                data = res.json()
                if data["result"]: # ë“±ë¡ë˜ì–´ ìˆìŒ
                    already_collected_count += 1
                    continue

                if href not in post_links:
                    post_links.add(href)
                    links.append(href)

        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        await asyncio.sleep(pause)

        # ìƒˆë¡œìš´ ì½˜í…ì¸  ë¡œë”©ëëŠ”ì§€ í™•ì¸
        new_height = await page.evaluate("document.body.scrollHeight")
        if new_height == last_height:
            # ë” ì´ìƒ ëŠ˜ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
            break
        last_height = new_height

        # await page.evaluate("window.scrollBy(0, Math.max(400, window.innerHeight*0.9));")
        # try:
        #     await page.wait_for_load_state("networkidle", timeout=3000) # ìŠ¤í¬ë¡¤ í›„ ëŒ€ê¸°(ìµœëŒ€)
        # except:
        #     pass
        # await asyncio.sleep(pause)
        #
        # if len(post_links) == last_count:
        #     stable_rounds += 1
        #     if stable_rounds >= 3:
        #         break
        # else:
        #     stable_rounds = 0
        #     last_count = len(post_links)

    # return sorted(post_links)
    # links.reverse() # ì—­ìˆœìœ¼ë¡œ ë’¤ì§‘ê¸°
    # return links

    rev_links = links[::-1]   # slicing, ì›ë³¸ ë³´ì¡´
    return rev_links

# ì»¨í…Œì´ë„ˆ: section > main > div:first-child > div:first-child > div[role="presentation"] > ul > li > img
UL_SEL   = "section main > div:nth-of-type(1) > div:nth-of-type(1) div[role='presentation'] ul"
LI_SEL   = UL_SEL + " > li"
IMG_SEL  = UL_SEL + " > li img"
FALLBACK_IMG  = "section main > div:nth-of-type(1) > div:nth-of-type(1) > div:nth-of-type(1) > div:nth-of-type(1) img"
NEXT_BTN_SEL  = (
    "section main > div:nth-of-type(1) > div:nth-of-type(1) "
    "button[aria-label*='ë‹¤ìŒ'], :has(button:has-text('ë‹¤ìŒ')), "
    "button[aria-label*='Next'], :has(button:has-text('Next'))"
)

# ê³µí†µ ë£¨íŠ¸
ROOT_MAIN   = "section main > div:nth-of-type(1) > div:nth-of-type(1)"
ROOT_DIALOG = "div[role='dialog']"

async def _resolve_root(page) -> Tuple[Optional[str], Optional[str]]:
    """ë©”ì¸ ë£¨íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ ë‹¤ì´ì–¼ë¡œê·¸ ë£¨íŠ¸ ì„ íƒ. (root_css, kind) ë°˜í™˜"""
    try:
        await page.wait_for_selector(ROOT_MAIN, timeout=2500)
        if await page.locator(ROOT_MAIN).count():
            return ROOT_MAIN, "main"
    except Exception:
        pass
    try:
        await page.wait_for_selector(ROOT_DIALOG, timeout=4000)
        if await page.locator(ROOT_DIALOG).count():
            return ROOT_DIALOG, "dialog"
    except Exception:
        pass
    return None, None

def _under(root: str, rel: str) -> str:
    """ë£¨íŠ¸ ì•„ë˜ ìƒëŒ€ ì…€ë ‰í„°ë¥¼ ì ˆëŒ€ ì…€ë ‰í„°ë¡œ"""
    if not rel:
        return root
    return f"{root} {rel}".strip()

def _sel(root: str, kind: str, rel_main: str, rel_dialog: Optional[str] = None) -> str:
    """ë£¨íŠ¸ ì¢…ë¥˜ì— ë”°ë¼ ì•Œë§ì€ ìƒëŒ€ ì…€ë ‰í„°ë¥¼ ì„ íƒ"""
    rel = rel_main if kind == "main" else (rel_dialog if rel_dialog is not None else rel_main)
    return _under(root, rel)

async def extract_imgs_src_only(page, post_url: str, seen: Set[str]) -> None:
    root, kind = await _resolve_root(page)

    # ë£¨íŠ¸/ì¢…ë¥˜ì— ë§ì¶° ëª¨ë“  ì…€ë ‰í„° êµ¬ì„±
    if root:
        # ìºëŸ¬ì…€ UL/LI/IMG
        UL_SEL  = _sel(root, kind, "div[role='presentation'] ul")
        IMG_SEL = _sel(root, kind, "div[role='presentation'] ul > li img")

        # ë‹¤ìŒ ë²„íŠ¼ (ë²„íŠ¼ "ìì²´"ë§Œ ì„ íƒ)
        NEXT_BTN_SEL = ", ".join([
            _sel(root, kind, "button[aria-label*='ë‹¤ìŒ']"),
            _sel(root, kind, "button:has-text('ë‹¤ìŒ')"),
            _sel(root, kind, "button[aria-label*='Next']"),
            _sel(root, kind, "button:has-text('Next')"),
        ])

        # fallback IMG (ë£¨íŠ¸ ê¸°ì¤€)
        FALLBACK_IMG = _sel(root, kind,
                            rel_main="> div:nth-of-type(1) > div:nth-of-type(1) img",
                            rel_dialog="img")
    else:
        UL_SEL = IMG_SEL = NEXT_BTN_SEL = None
        # ë£¨íŠ¸ë¥¼ ëª» ì°¾ìœ¼ë©´ ë‘˜ ë‹¤ ì»¤ë²„
        FALLBACK_IMG = (
            "section main > div:nth-of-type(1) > div:nth-of-type(1) "
            "> div:nth-of-type(1) > div:nth-of-type(1) img, "
            "div[role='dialog'] img"
        )

    # 1) UL ëŒ€ê¸° (ìˆìœ¼ë©´ ìºëŸ¬ì…€ ëª¨ë“œ, ì—†ìœ¼ë©´ fallback)
    ul_found = False
    if UL_SEL:
        try:
            await page.wait_for_selector(UL_SEL, timeout=5000)
            ul_found = True
        except Exception:
            ul_found = False

    # ê³µí†µ: ì´ë¯¸ì§€ í•œ ë²ˆì— ìµœëŒ€ í•´ìƒë„ë§Œ ìˆ˜ì§‘
    async def collect_max_images(selector: str) -> List[str]:
        try:
            n = await page.locator(selector).count()
            if n > 0:
                await page.locator(selector).nth(n - 1).scroll_into_view_if_needed()
        except Exception:
            pass

        urls: List[str] = await page.evaluate(
            """
            (sel) => {
              const pickLargestFromSrcset = (img) => {
                const ss = img.getAttribute('srcset');
                if (!ss) return null;
                const items = ss.split(',').map(s => s.trim()).map(entry => {
                  const [u, d] = entry.split(/\\s+/);
                  let w = 0;
                  if (d && d.endsWith('w')) {
                    const n = parseInt(d.slice(0, -1), 10);
                    if (!isNaN(n)) w = n;
                  }
                  return { url: u, w };
                }).filter(it => it.url);
                if (!items.length) return null;
                items.sort((a,b)=>b.w-a.w);
                return items[0].url;
              };

              const preferAttrs = (img) => {
                const cand = [
                  'data-src-large','data-src-2x','data-large','data-original',
                  'data-srcset','data-fullsrc','data-full','data-url'
                ];
                for (const a of cand) {
                  const v = img.getAttribute(a);
                  if (v) return v;
                }
                return null;
              };

              const imgs = Array.from(document.querySelectorAll(sel));
              return imgs.map(img =>
                pickLargestFromSrcset(img) ||
                preferAttrs(img) ||
                img.currentSrc ||
                img.getAttribute('src')
              ).filter(Boolean);
            }
            """,
            selector
        )
        return urls

    # 2) ìºëŸ¬ì…€
    if ul_found and IMG_SEL and NEXT_BTN_SEL:
        async def collect_from_main() -> None:
            for u in await collect_max_images(IMG_SEL):
                if u:
                    seen.add(u)

        await collect_from_main()

        # "ë‹¤ìŒ" í´ë¦­ ë°˜ë³µ (í´ë¦­ ì „ ìˆ˜ì§‘ â†’ í´ë¦­ â†’ ì ê¹ ëŒ€ê¸°)
        for _ in range(20):
            next_btn = page.locator(NEXT_BTN_SEL).first
            if not await next_btn.count():
                break

            # í´ë¦­ ì „ ìˆ˜ì§‘(í˜„ì¬ í™”ë©´)
            await collect_from_main()

            # í´ë¦­
            try:
                await next_btn.click(timeout=1000)
            except Exception:
                try:
                    await next_btn.click(timeout=1000, force=True)
                except Exception:
                    break

            # await asyncio.sleep(0.5) # ì‹œê°„ ì¡°ì •
            await asyncio.sleep(0.7) # ì‹œê°„ ì¡°ì •

        # ë§ˆì§€ë§‰ í•œ ë²ˆ ë”
        await collect_from_main()

    # 3) Fallback
    if await page.locator(FALLBACK_IMG).count():
        for u in await collect_max_images(FALLBACK_IMG):
            if u:
                seen.add(u)

    # ===== 4) (ì„ íƒ) ì¶”ê°€ ì¤‘ë³µ ì¶•ì†Œ: ì‚¬ì´ì¦ˆ íŒŒë¼ë¯¸í„° ë¬´ì‹œ í‚¤ë¡œ ë³‘í•© =====
    # ê°™ì€ ì‚¬ì§„ì¸ë° ?w=, ?h=, =s2048 ë“±ë§Œ ë‹¤ë¥¸ ê²½ìš°ë¥¼ ë” ì¤„ì´ê³  ì‹¶ë‹¤ë©´ ì‚¬ìš©
    import re
    def norm(u: str) -> str:
        # ì•„ì£¼ ë³´ìˆ˜ì ìœ¼ë¡œ ëª‡ ê°€ì§€ ì‚¬ì´ì¦ˆ í† í°ë§Œ ì œê±°
        #  - Google ê³„ì—´: "=s1234" ê¼¬ë¦¬ í† í°
        u2 = re.sub(r'(=s\d+)(?=$)', '', u)
        #  - í”í•œ width/height ì¿¼ë¦¬íŒŒë¼ë¯¸í„° ì œê±° (ë‹¤ë¥¸ í† í° ë³´ì¡´)
        u2 = re.sub(r'([?&])(w|width|h|height|size|s)=\d+(?=(&|$))', r'\1', u2, flags=re.I)
        u2 = re.sub(r'[?&]+$', '', u2)
        return u2

    best_by_key = {}
    for u in seen:
        key = norm(u)
        # ê°™ì€ í‚¤ë©´ ë” ê¸´(ëŒ€ì²´ë¡œ ê³ í•´ìƒë„) URLì„ ë³´ì¡´í•˜ëŠ” ê°„ë‹¨ ê·œì¹™
        if key not in best_by_key or len(u) > len(best_by_key[key]):
            best_by_key[key] = u

    final_urls = list(best_by_key.values())

    # ë¡œê·¸
    # print(f"\n=== {post_url} ===")
    # for i, u in enumerate(final_urls, 1):
    #     print(f"[IMG {i}] {u}")

    return final_urls


def looks_like_good_video(resp):
    """ì¢‹ì€(ì™„ì „) mp4 ì‘ë‹µë§Œ í†µê³¼: 200 OK, video/*, CDN host, /o1/, ì¶©ë¶„í•œ content-length, content-range ì—†ìŒ"""
    try:
        if resp.status != 200:
            return False
        # content-type
        ct = (resp.headers or {}).get("content-type", "")
        if "video" not in ct:
            return False
        # ì¡°ê° ìŠ¤íŠ¸ë¦¬ë° ì œì™¸
        hdrs = {k.lower(): v for k, v in (resp.headers or {}).items()}
        if "content-range" in hdrs:
            return False
        # CDN host + ê²½ë¡œ ê·œì¹™
        if not (CDN_HOST_RE.match(resp.url) and CDN_PATH_ALLOW.search(resp.url)):
            return False
        # í¬ê¸°(ìˆìœ¼ë©´ ì²´í¬)
        clen = hdrs.get("content-length")
        if clen and clen.isdigit() and int(clen) < MIN_GOOD_VIDEO_BYTES:
            return False
        return True
    except Exception:
        return False

async def force_play_video_if_possible(page):
    """ë¹„ë””ì˜¤ ë³´ì´ê²Œ í•˜ê³  ì¬ìƒ ìœ ë„ â†’ ë„¤íŠ¸ì›Œí¬ íŠ¸ë¦¬ê±°"""
    vid = page.locator("section main > div:nth-of-type(1) > div:nth-of-type(1) > div:nth-of-type(1) > div:nth-of-type(1) video").first
    if await vid.count():
        try:
            await vid.scroll_into_view_if_needed()
        except:
            pass
    # ì¬ìƒ ë²„íŠ¼ í´ë¦­ ì‹œë„
    try:
        play_btn = page.locator("button[aria-label*='Play'], button:has-text('Play')")
        if await play_btn.count():
            await play_btn.click()
            # await asyncio.sleep(0.5)
            await asyncio.sleep(0.7)
    except:
        pass
    # JSë¡œ ê°•ì œ ì¬ìƒ
    try:
        await page.evaluate("""
            (sel)=>{ const v=document.querySelector(sel);
                     if(v){ v.muted=true; v.play().catch(()=>{}); } }
        """, "article video")
    except:
        pass

# https://scontent-ssn1-1.cdninstagram.com/v/t51
async def extract_media_from_post(page, url: str):
    """
    ì–´ë–¤ URLì´ë“ (í¬ìŠ¤íŠ¸/ë¦´ìŠ¤) ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ë™ì‹œì— ìˆ˜ì§‘.
    - images: DOMì˜ <img>ì—ì„œ ìˆ˜ì§‘(+ìºëŸ¬ì…€ next)
    - video_cdn: ë„¤íŠ¸ì›Œí¬ ì‘ë‹µì—ì„œ 'ì¢‹ì€ mp4'ë§Œ ì¦‰ì‹œ ì„ ë³„(í•„ìš” ì‹œ DOMì˜ <video src>ë„ ë³´ì¡°)
    """
    good_video_urls: Set[str] = set()

    # ë„¤íŠ¸ì›Œí¬ ì‘ë‹µì—ì„œ 'ì¢‹ì€' ë¹„ë””ì˜¤ë§Œ ì¦‰ì‹œ ì„ ë³„ ì €ì¥
    def on_response(resp):
        if looks_like_good_video(resp):
            good_video_urls.add(resp.url)
    page.on("response", on_response)

    await page.goto(url, wait_until="domcontentloaded")
    await asyncio.sleep(DELAY_SECOND) # ì‹œê°„ ì¡°ì •

    # ì´ë¯¸ì§€ ìˆ˜ì§‘ (ë„ìš°ë¯¸ ì‚¬ìš©)
    seen = set()
    images: List[str] = await extract_imgs_src_only(page, url, seen)

    # ë¹„ë””ì˜¤ ìˆ˜ì§‘: ë³´ì´ê²Œ/ì¬ìƒ ìœ ë„ â†’ 'ì¢‹ì€' ì‘ë‹µ ëŒ€ê¸°
    await force_play_video_if_possible(page)

    # ì ì‹œ ì¬ì‹œë„í•˜ë©° ì‘ë‹µ ëª¨ìœ¼ê¸°(ì´ ~6ì´ˆ)
    for _ in range(4):
        if good_video_urls:
            break
        await asyncio.sleep(0.75) # ì‹œê°„ ì¡°ì •

    # ë³´ì¡°: DOMì˜ <video src>ë„ ìˆ˜ì§‘(ìˆì„ ìˆ˜ ìˆìŒ) â†’ CDN í•„í„° ì ìš©
    vids = page.locator("section main > div:nth-of-type(1) video")
    n_vid = await vids.count()
    dom_video_srcs = []
    for i in range(n_vid):
        vsrc = await vids.nth(i).get_attribute("src")
        if vsrc and vsrc.startswith("http"):
            dom_video_srcs.append(vsrc)

    # DOM/ë„¤íŠ¸ì›Œí¬ í•©ì¹˜ê³ , ìµœì¢…ì ìœ¼ë¡œ CDN ê·œì¹™ìœ¼ë¡œ í•„í„°
    candidates = set(dom_video_srcs) | good_video_urls
    # video_cdn = sorted(u for u in candidates if CDN_HOST_RE.match(u) and CDN_PATH_ALLOW.search(u))
    unique = {}
    for u in candidates:
        if CDN_HOST_RE.match(u) and CDN_PATH_ALLOW.search(u):
            k = canonical_cdn_key(u)
            # ë¨¼ì € ë³¸(ê°€ì¥ 'ì¢‹ì•„ ë³´ì´ëŠ”') ì›ë³¸ URLì„ ë³´ì¡´
            unique.setdefault(k, u)
    video_cdn = sorted(unique.values())

    return {
        "post_url": url,
        "images": images,        # í•­ìƒ ì±„ì›€(ìˆìœ¼ë©´)
        "videos": [],            # ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€
        "video_cdn": video_cdn,  # í•­ìƒ ì±„ì›€(ìˆìœ¼ë©´)
    }

# async def extract_media_from_post(page, url: str) -> Dict[str, Any]:
#     """ê°œë³„ í¬ìŠ¤íŠ¸/ë¦´ìŠ¤ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€/ì˜ìƒ URL ì¶”ì¶œ."""
#     collected_network: Set[str] = set()
#
#     def on_response(resp):
#         # blob: ì œì™¸, ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì‘ë‹µë§Œ ìˆ˜ì§‘
#         ct = (resp.headers or {}).get("content-type", "")
#         if ("image" in ct or "video" in ct) and not resp.url.startswith("blob:"):
#             collected_network.add(resp.url)
#
#     page.on("response", on_response)
#
#     await page.goto(url, wait_until="domcontentloaded")
#     await asyncio.sleep(2.5)
#
#     # DOM ê¸°ë°˜ ìˆ˜ì§‘
#     images: List[str] = await extract_imgs_src_only(page, url)   # â† ë°˜ë“œì‹œ await
#     videos: List[str] = []
#     imgs = page.locator("article img")                      # â† ìºëŸ¬ì…€ ë£¨í”„ì—ì„œ ì‚¬ìš©í•  locator
#     vids = page.locator("article video")
#
#     # ë¹„ë””ì˜¤(src)
#     n_vid = await vids.count()
#     for i in range(n_vid):
#         vsrc = await vids.nth(i).get_attribute("src")
#         if vsrc and vsrc.startswith("http"):
#             videos.append(vsrc)
#
#     # ìºëŸ¬ì…€ 'ë‹¤ìŒ' í´ë¦­ ì‹œë„ (ìµœëŒ€ 5íšŒ)
#     for _ in range(5):
#         next_btn = page.locator("button[aria-label*='Next'], button:has-text('Next')")
#         if not await next_btn.count():
#             break
#         try:
#             await next_btn.click()
#             await asyncio.sleep(1.5)
#
#             # ìƒˆë¡œ ë¡œë“œëœ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ë‹¤ì‹œ ìˆ˜ì§‘
#             n_img = await imgs.count()
#             for i in range(n_img):
#                 src = await imgs.nth(i).get_attribute("src")
#                 if src and src.startswith("http"):
#                     images.append(src)
#
#             n_vid = await vids.count()
#             for i in range(n_vid):
#                 vsrc = await vids.nth(i).get_attribute("src")
#                 if vsrc and vsrc.startswith("http"):
#                     videos.append(vsrc)
#         except Exception:
#             break
#
#     # ì¤‘ë³µ ì œê±°
#     images = list(dict.fromkeys(images))
#     videos = list(dict.fromkeys(videos))
#
#     return {
#         "post_url": url,
#         "images": images,
#         "videos": videos,
#         "video_cdn": filtered_video_cdn,
#     }


_seq = itertools.count()
_seq_lock = asyncio.Lock()

def now_ms() -> int:
    return int(time.time() * 1000)  # ì‚¬ëŒ ì½ê¸° ì‰¬ìš´ ë²½ì‹œê³„ ms

async def next_seq() -> int:
    # ë™ì¼ ms ì¶©ëŒ ë° ì½”ë£¨í‹´ ë™ì‹œì„± ëŒ€ë¹„
    async with _seq_lock:
        return next(_seq)

def safe_open_exclusive(path: str):
    # ì¡´ì¬ ì¶©ëŒ ì‹œ ì—ëŸ¬ë¡œ ì‹¤íŒ¨ì‹œí‚¤ëŠ” ì „ìš© ì˜¤í”ˆ (ë®ì–´ì“°ê¸° ë°©ì§€)
    return open(path, "xb")

# ======== ë‹¤ìš´ë¡œë“œ ========
async def download_one(session: aiohttp.ClientSession, url: str, save_dir: str, prefix: str="media") -> Optional[str]:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=60)) as resp:
            if resp.status != 200:
                return None
            ct = resp.headers.get("Content-Type", "")
            ext = guess_ext_from_url_or_type(url, ct)

            ts = now_ms()
            seq = await next_seq()  # íƒ€ì´ë¸Œë ˆì´ì»¤
            fname = sanitize_filename(f"{prefix}_{ts:013d}_{seq:06d}{ext}")
            # fname = sanitize_filename(f"{prefix}_{int(time.time()*1000)}{ext}")
            path = os.path.join(save_dir, fname)
            data = await resp.read()
            with safe_open_exclusive(path) as f:
                f.write(data)
            # with open(path, "wb") as f:
            #     f.write(data)
            return path
    except FileExistsError:
        # ê·¹íˆ ë“œë¬¸ ê²½í•© ëŒ€ë¹„ (ë‹¤ì‹œ í•œ ë²ˆ ì‹œí€€ìŠ¤ ì¦ê°€)
        seq = await next_seq()
        ts = now_ms()
        alt = os.path.join(save_dir, sanitize_filename(f"{prefix}_{ts:013d}_{seq:06d}{ext}"))
        with open(alt, "wb") as f:
            f.write(data)
        return alt
    except Exception:
        return None


async def download_media(images: List[str], videos: List[str], video_cdn: List[str], dirs: Dict[str, str], account: str):
    # ë¹„ë””ì˜¤ëŠ” video_cdnë§Œ ì‚¬ìš© (ìš”êµ¬ì‚¬í•­)
    video_sources = video_cdn if video_cdn else []

    # ì¤‘ë³µ ì œê±°
    images = list(dict.fromkeys(images))
    video_sources = list(dict.fromkeys(video_sources))

    conn = aiohttp.TCPConnector(limit_per_host=MAX_CONCURRENCY, ssl=False)
    async with aiohttp.ClientSession(connector=conn) as session:
        tasks = []
        for i, url in enumerate(images, 1):
            tasks.append(download_one(session, url, dirs["images"], prefix=f"{account}_img"))
        for i, url in enumerate(video_sources, 1):
            tasks.append(download_one(session, url, dirs["reels"], prefix=f"{account}_reel"))

        results = await asyncio.gather(*tasks, return_exceptions=True)
        ok_paths = [p for p in results if isinstance(p, str) and p]

        # ğŸ”¥ ë¹„ë””ì˜¤ ê¸¸ì´ê°€ 1ì´ˆ ë¯¸ë§Œì´ê±°ë‚˜(0 í¬í•¨) ê¸¸ì´ íŒë… ì‹¤íŒ¨(None)ë©´ ì‚­ì œ
        deleted = []
        for p in ok_paths:
            try:
                if os.path.exists(p) and p.startswith(dirs["reels"]):
                    dur = get_video_duration_sec(p)  # Noneì´ë©´ íŒë… ì‹¤íŒ¨
                    # ê·œì¹™: dur is None(ê¸¸ì´ ë¶ˆëª…) ë˜ëŠ” dur < 1.0 â†’ ì‚­ì œ
                    if dur is None or dur < 1.0:
                        os.remove(p)
                        deleted.append(p)
            except Exception as e:
                print(f"[WARN] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {p}, {e}")

        if deleted:
            print(f"[INFO] {len(deleted)}ê°œì˜ ì§§ì€/ê¸¸ì´ë¶ˆëª… ë¹„ë””ì˜¤ ì‚­ì œë¨ (<1s or unknown)")

        # ì‚­ì œë˜ì§€ ì•Šì€ ê²½ë¡œë§Œ ë°˜í™˜
        return [p for p in ok_paths if p not in deleted]

def log_targets(context, tag=""):
    pages = context.pages
    bgs = context.background_pages
    sws = context.service_workers
    print(f"[DEBUG]{tag} pages={len(pages)} bg_pages={len(bgs)} service_workers={len(sws)}")
    for p in pages:
        try:
            print("  - page:", p.url)
        except Exception:
            pass


# ======== ë©”ì¸ í”Œë¡œìš° ========
async def handle_account(context, account: str):
    page = await context.new_page()
    await ensure_login(page)
    await go_to_profile(page, account)

    if account == 'test':
        # ë””ë²„ê¹…
        links = TEST_LINKS
    else:
        # ë§í¬ ìˆ˜ì§‘
        links = await collect_post_links(page)


    if len(links) > 5:
        print(f"[{account}] Collect Postlinks: {len(links)}")
    if len(links) > 300:
        await asyncio.sleep(60 * 30)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€

    # ì €ì¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
    dirs = ensure_dirs(BASE_SAVE_DIR, account)

    all_saved = []
    check_saved = []
    cnt = 0
    for idx, link in enumerate(links, 1):
        today = datetime.datetime.today().strftime('%Y/%m/%d %H:%M:%S')
        # print(f"[{today}] [{account}] ({idx}/{len(links)}) {link}")
        print(f"[{account}] ({idx}/{len(links)}) {link}")

        parsed = urlparse(link)
        # ParseResult(scheme='https', netloc='www.instagram.com', path='/fkaus014/p/DO27U_DDw63/')
        qs_link = normalize_ig_post_url(parsed.path)
        parsed = urlparse(qs_link)

        cnt += 1
        # None, False, 0, '', [], {}, ëª¨ë‘ ì—¬ê¸°ë¡œ ë“¤ì–´ì˜´
        # if idx == 5: # ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ìš©
        #     break
        try:
            media = await extract_media_from_post(page, link)
            # print('media', media)
            # ì´ë¯¸ì§€: ê·¸ëŒ€ë¡œ / ë¹„ë””ì˜¤: VIDEO_PREFIXë§Œ
            saved = await download_media(
                media["images"], media["videos"], media["video_cdn"], dirs, account
            )
            # print('saved', saved)
            all_saved.extend(saved or [])
            check_saved.extend(saved or [])
            if idx % 10 == 0:
                today = datetime.datetime.today().strftime('%Y/%m/%d %H:%M:%S')
                print(f"[{account}] [{today}] Interim check of number of saved files : {len(check_saved)}")
                check_saved = []
            link_segment = extract_account_and_type(normalize_ig_post_url(link))
            try:
                requests.post(
                    'https://chickchick.shop/func/scrap-posts',
                    json={
                        "account": str(account),
                        "post_urls": link,
                        "type": link_segment["type"],
                    },
                    # timeout=5
                    timeout=(3, 20)  # (connect_timeout=3ì´ˆ, read_timeout=20ì´ˆ)
                )
            except Exception as e:
                # logging.warning(f"progress-update ìš”ì²­ ì‹¤íŒ¨: {e}")
                print(f"progress-update ìš”ì²­ ì‹¤íŒ¨-1: {e}")
                pass  # ì˜¤ë¥˜
            await asyncio.sleep(DELAY_SECOND)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
            if cnt % 300 == 0:
                await asyncio.sleep(60 * 30)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
            elif cnt % 100 == 0:
                await asyncio.sleep(DELAY_MINUTE)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
        except Exception as e:
            print(f"  -> ì—ëŸ¬: {e}")

    await page.close()
#     log_targets(context, tag=f" after {account}")

    if len(links) > 0:
        print(f"[{account}] Number of files saved: {len(all_saved)}")

    return len(links)

async def run_scrap():
    nowTime = datetime.datetime.today().strftime('%Y/%m/%d %H:%M:%S')
    print(f'        {nowTime}: running scrap ig')
    async with async_playwright() as pw:
        context = await pw.chromium.launch_persistent_context(
            USER_DATA_DIR,
            # headless=HEADLESS,  # ì£¼ì„í•˜ë©´ ë¸Œë¼ìš°ì € off
            # viewport={"width": 1920, "height": 1080},
            args=["--disable-blink-features=AutomationControlled", "--no-sandbox"],
        )

        # ë””ë²„ê¹…
        # await handle_account(context, 'test')

        for i, acc in enumerate(ACCOUNTS):
            today = datetime.datetime.today().strftime('%Y/%m/%d %H:%M:%S')
            print(f"\n=== [{today}] Start account processing: {acc} ({i+1}/{len(ACCOUNTS)}) ===")
            try:
                rs = await handle_account(context, acc)
            except Exception as e:
                print(f"[{acc}] Error in processing: {e}")
                continue

            if i < len(ACCOUNTS) - 1:
                if rs > 30:
                    await asyncio.sleep(DELAY_MINUTE) # ê³„ì • ê°„ ì¿¨ë‹¤ìš´(ì„ íƒ): ê³¼ë„í•œ ì ‘ê·¼ ë°©ì§€
                else:
                    await asyncio.sleep(30)

        await context.close()

def run_scrap_job():
    # ì´ë²¤íŠ¸ ë£¨í”„ëŠ” â€œìŠ¤ë ˆë“œë‹¹ 1ê°œâ€ê°€ ì›ì¹™
    asyncio.run(run_scrap())

if __name__ == "__main__":
    '''
    asyncio.run(run_scrap)      # âŒ í•¨ìˆ˜ ìì²´ë¥¼ ë„˜ê¹€
    asyncio.run(run_scrap())    # âœ… ì½”ë£¨í‹´ ê°ì²´ë¥¼ ë„˜ê¹€
    '''
    asyncio.run(run_scrap())
